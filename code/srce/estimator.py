'''
    this version select RO based on the testing queries generated by geometric distribution
'''
import math
import pickle
import numpy as np
import time
import utility_functions
# from generate_ros import sampling, simple_clustering

import sys
dataset = sys.argv[1]
sample_size = 120
label = 'testing'
sel_size = 30

start_p = 6

start_offset = np.load(f'{dataset}_start_dists-i2-{sample_size}_{sel_size}_{start_p}.npy')

D = np.load(f'{dataset}_D_i2_{sample_size}_{sel_size}_{start_p}.npy')
with open(f'{dataset}_pivots-cluster-i2-{sample_size}_{sel_size}_{start_p}.pickle', 'rb') as file:
    pivots_all = pickle.load(file)
with open(f'{dataset}_cards-cluster-i2-{sample_size}_{sel_size}_{start_p}.pickle', 'rb') as file:
    cards_all = pickle.load(file)


testing_data = np.load(f'../data/{dataset}/{dataset}_{label}Data.npy')
occs = np.load(f'../data/{dataset}/{dataset}_{label}_tss_occ.npy')
tss = np.load(f'../data/{dataset}/{dataset}_{label}_tss.npy')
# testing_data = np.load(f'/research/local/hai/ce4hd/dataset/{dataset}_{label}Data.npy')
# occs = np.load(f'/research/local/hai/ce4hd/dataset/{dataset}_{label}_tss_occ.npy')
# tss = np.load(f'/research/local/hai/ce4hd/dataset/{dataset}_{label}_tss.npy')
order = np.load(f'{dataset}_orders-i2-{sample_size}_{sel_size}_250_{start_p}.npy')
K = 250

fds = np.load(f'{dataset}_top_{K}.npy')
def get_est(t, lcards, ltaus):
    if t < ltaus[0]:
        return lcards[0]
    for j in range(len(ltaus) - 1):
        if t >= ltaus[j] and t < ltaus[j+1]:
            est = lcards[j] + (lcards[j+1] - lcards[j])*(t - ltaus[j])/(ltaus[j+1] - ltaus[j])
            return est
    return lcards[-1]


# from sklearn.metrics import *
def qerror_minmax(labels, predictions, print_info=False):
    max_values = np.maximum(labels, predictions)
    min_values = np.minimum(labels, predictions)
    q_error = max_values / min_values
    if print_info:
        print("{:.2f}\n{:.2f}\n{:.2f}\n{:.2f}\n{:.2f}\n{:.2f}\n{:.2f}\n{:.2f}\n".format(np.mean(q_error),
                                                                        np.percentile(q_error, 25),
                                                                        np.percentile(q_error, 50),
                                                                        np.percentile(q_error, 75),
                                                                        np.percentile(q_error, 90),
                                                                        np.percentile(q_error, 95),
                                                                        np.percentile(q_error, 99),
                                                                        np.max(q_error)))
    # mae = mean_absolute_error(labels, predictions)
    # x = np.abs(labels - predictions)
    e = np.mean(q_error)
    # e = np.mean(x)
    return e, e

def mean_abs(labels, predictions):
    x = np.abs(labels - predictions)
    e = np.mean(x)
    return e, e

def get_curve_and_shift(self_start, QS):
    best_id = -1
    best_error = 999999999.0
    best_shift = -1
    # for _id in range(sample_size):
    _errors = []
    for _id in range(sel_size):
        _est = []
        _gt = []
        _pivots = pivots_all[_id]
        _cards = cards_all[_id]
        _shift = self_start-start_offset[_id]
        for j in range(QS.shape[0]):
            if 1.0 < QS[j][1]: # select shape based on 100
                _gt.append(QS[j][1])
                _est.append(get_est(QS[j][0] - _shift, _cards, _pivots)+2)
                # _est.append(get_est(QS[j][0], _cards, _pivots)+2)


        _error, _ = qerror_minmax(np.array(_gt)+0.6, np.array(_est)+0.6)
        _errors.append(_error)
        if _error < best_error:
            best_error = _error
            best_id = _id
            best_shift = _shift
    return best_error, best_id, best_shift

# import math
def get_test_error(_pid, self_start, QS):
    _est = []
    _gt = []
    _pivots = pivots_all[_pid]
    _cards = cards_all[_pid]
    _shift = self_start - start_offset[_pid]
    for j in range(QS.shape[0]):
        if 1.0 < QS[j][1] <= 300 and j % 15 == 0:  # select shape based on 100
            # _gt.append(QS[j][0] + 0.6)
            # _est.append(get_est(QS[j][1], _pivots, _cards) + _shift + 0.6)
            _gt.append(QS[j][1]+ 0.6)
            _est.append(get_est(QS[j][0] - _shift, _cards, _pivots)+start_p+ 0.6)

    _error, _ = qerror_minmax(np.array(_gt), np.array(_est))
    # _error = 0
    return _error, _shift





def gen_mapping():
    # test_ids1 = [5 * _i_ for _i_ in range(13)]
    # id_mappings = {}
    # for _id_ in test_ids1:
    #     if _id_ == 0:
    #         id_mappings[0] = [1, 2, 3, 4]
    #     elif _id_ == 60:
    #         id_mappings[60] = [59, 58, 57, 56]
    #     else:
    #         id_mappings[_id_] = [_id_-4, _id_ -3, _id_ - 2, _id_ - 1, _id_ + 1, _id_ + 2, _id_ + 3, _id_+4]

    # for _id_ in test_ids1:
    #     if _id_ == 0:
    #         id_mappings[0] = [1, 2]
    #     elif _id_ == 60:
    #         id_mappings[60] = [59, 58]
    #     else:
    #         id_mappings[_id_] = [_id_ - 2, _id_ - 1, _id_ + 1, _id_ + 2]
    test_ids1 = [5 * _i_ for _i_ in range(7)]
    id_mappings = {}
    for _id_ in test_ids1:
        if _id_ == 0:
            id_mappings[0] = [1, 2, 3, 4]
        elif _id_ == 30:
            id_mappings[30] = [26, 27, 28, 29]
        else:
            id_mappings[_id_] = [_id_ - 4, _id_ - 3, _id_ - 2, _id_ - 1, _id_ + 1, _id_ + 2, _id_ + 3, _id_ + 4]
    return test_ids1, id_mappings


test_ids1, id_mappings = gen_mapping()
def get_curve_and_shift_based_on_shape_opt1(self_start, QS):

    # test_ids1 = [0, 5, 10, 15, 20, 25]
    # id_mappings = {0: [1,2,3,4], 5:[1,2,3,4,6,7,8,9],
    #              10: [6,7,8,9,11,12,13,14],
    #              15: [11,12,13,14,16,17,18,19],
    #              20: [16,17,18,19,21,22,23,24],
    #              25: [21,22,23,24,26,27,28,29]}


    best_id = -1
    best_order = -1
    best_error = 999999999.0
    best_shift = -1
    # for _id in range(sample_size):
    # _errors = []
    for _id in test_ids1:
        _error, _shift = get_test_error(order[_id], self_start, QS)
        if _error < best_error:
            best_error = _error
            best_id = order[_id]
            best_shift = _shift
            best_order = _id


    test_ids2 = id_mappings[best_order]
    for _id in test_ids2:
        _error, _shift = get_test_error(order[_id], self_start, QS)
        if _error < best_error:
            best_error = _error
            best_id = order[_id]
            best_shift = _shift
            best_order = _id

    return best_error, best_id, best_shift


def get_curve_and_shift_based_on_shape(self_start, QS, xk):
    best_id = -1
    best_error = 999999999.0
    best_shift = -1
    # for _id in range(sample_size):
    # _errors = []
    for _id in range(sel_size):
        _est = []
        _gt = []
        _pivots = pivots_all[_id]
        _cards = cards_all[_id]
        _shift = self_start - start_offset[_id]
        for j in range(QS.shape[0]):
            if 1.0 < QS[j][1] <= xk and xk < 100:  # select shape based on 100
                # _gt.append(QS[j][0])
                # _est.append(get_est(QS[j][1], _pivots, _cards) + _shift)
                _gt.append(QS[j][1]+ 0.6)
                _est.append(get_est(QS[j][0]-_shift, _cards, _pivots)+start_p+ 0.6)
            elif 1.0 < QS[j][1] <= xk and j % 5 ==0:
                _gt.append(QS[j][1] + 0.6)
                _est.append(get_est(QS[j][0] - _shift, _cards, _pivots) + start_p + 0.6)

        _error, _ = qerror_minmax(np.array(_gt), np.array(_est))
        # best_error = 0.0 #_error
        # best_id = _id
        # best_shift = _shift
        if _error < best_error:
            best_error = _error
            best_id = _id
            best_shift = _shift
    return best_error, best_id, best_shift




# maintain last two points shape

def process_Q(xk):
    mean_qerrors = []
    best_ids = []
    real_queries = []
    start_i = 0
    _gtg = []
    _estg = []

    cards = [i + 1 for i in range(K)]
    cards = np.array(cards)

    eval_time = 0.0
    import time
    for i in range(tss.shape[0]):
        if i % 1000 == 0:
            print(i)
        # if i == 1780: # 11 15
        #     print(0)
        # self_start = testing_data[start_i+1][tss.shape[1]+1] # except face other will be testing_data[start_i+1]
        if occs[i] < 2:
            start_i += occs[i]
            continue
        # if self_start != 2.0:
        #     start_i += occs[i]
        #     print(f'---{i}')
        #     continue
        real_queries.append(i)
        _fds = fds[i]
        # self_start = testing_data[start_i+1][tss.shape[1]]
        self_start = _fds[start_p]
        qs = testing_data[start_i:start_i+occs[i], tss.shape[1]:tss.shape[1]+2]
        qs_s = np.column_stack((_fds[start_p:], cards[start_p:]))
        se = time.time()
        # _, best_id, shift = get_curve_and_shift_based_on_shape(self_start, qs_s, xk)
        _, best_id, shift =  get_curve_and_shift_based_on_shape_opt1(self_start, qs_s) #get_curve_and_shift(self_start, qs)
        ee = time.time()
        eval_time += (ee-se)

        _pivots = pivots_all[best_id]
        _cards = cards_all[best_id]
        best_ids.append(best_id)
        _gt = []
        _est = []
        for j in range(qs.shape[0]):
            if 1.0 < qs[j][1] :
                _gt.append(qs[j][1])
                _t = get_est(qs[j][0] - shift, _cards, _pivots)+start_p
                _est.append(_t)
                _gtg.append(qs[j][1])
                _estg.append(_t)
        _error = utility_functions.qerror_minmax(np.array(_gt) + 0.6, np.array(_est) + 0.6)
        mean_qerrors.append(_error)
        start_i += occs[i]
    _error = utility_functions.qerror_minmax(np.array(_gtg) + 0.6, np.array(_estg) + 0.6, print_info=True)
    print(len(_gtg),testing_data.shape)
    print(_error)
    # np.save(f'{dataset}_mean_errors-{sample_size}_{sel_size}_100.npy', np.array(mean_qerrors))
    np.save(f'{dataset}_best_ids-{sample_size}_{sel_size}_200_card.npy', np.array(best_ids))
    np.save(f'{dataset}_queries-{sample_size}_{sel_size}_card.npy', np.array(real_queries))
    np.save(f'{dataset}_mean_errors-{sample_size}_{sel_size}_card.npy', np.array(mean_qerrors))

    print(eval_time/tss.shape[0])




XK = [250]
for xxxk in XK:
    print(xxxk)
    process_Q(xxxk)
# get_topk_based_topk()